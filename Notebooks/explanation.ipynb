{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Import"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Feature Importance: Analyze the contribution of each feature to the final model’s predictions using coefficients and importance scores.\n",
    "* Precision-Recall Curve: Plot the Precision-Recall curve to evaluate the model’s performance in distinguishing between bankrupt and non-bankrupt companies.\n",
    "* SHAP Analysis: Use SHAP (SHapley Additive exPlanations) to visualize and explain individual predictions, highlighting key factors influencing bankruptcy risk.\n",
    "* Business Insights and Implications: Translate model findings into actionable insights for stakeholders, including recommendations for risk mitigation and investment strategies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Data\n",
    "* Trained Model: Final trained model (likely a Logistic Regression or Cox Proportional Hazards model)\n",
    "* Test Dataset: A held-out portion of your data not used during training\n",
    "* Feature Matrix: The processed features used for prediction (X_test)\n",
    "* Target Variable: The actual bankruptcy outcomes (y_test)\n",
    "* Prediction Scores: Probability outputs from your model on the test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Metrics Visualization: ROC Curves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import roc_curve, auc\n",
    "\n",
    "# Assuming you have:\n",
    "# model = your trained model\n",
    "# X_test = your test features\n",
    "# y_test = your test labels\n",
    "\n",
    "# Get prediction probabilities\n",
    "y_pred_proba = model.predict_proba(X_test)[:, 1]  # For binary classification\n",
    "\n",
    "# Calculate ROC curve points\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_proba)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "\n",
    "# Plot ROC curve\n",
    "plt.figure(figsize=(10, 8))\n",
    "plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('Receiver Operating Characteristic (ROC) Curve')\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.savefig('roc_curve.png', dpi=300, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ROC curve is particularly useful for your balanced dataset because it shows the model's performance across different classification thresholds, which is important when the default threshold (0.5) might not be optimal due to class imbalance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create ROC curve using the test data\n",
    "from sklearn.metrics import roc_curve, roc_auc_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Get probability predictions\n",
    "y_pred_prob = log_reg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculate ROC curve points\n",
    "fpr, tpr, thresholds = roc_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Calculate AUC\n",
    "auc = roc_auc_score(y_test, y_pred_prob)\n",
    "\n",
    "# Create plot\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.plot([0, 1], [0, 1], 'k--', label='Random (AUC = 0.5)')\n",
    "plt.plot(fpr, tpr, label=f'Logistic Regression (AUC = {auc:.3f})')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC Curve for Logistic Regression Bankruptcy Prediction')\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim([0, 1])\n",
    "plt.ylim([0, 1.05])\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When to Use Each:\n",
    "\n",
    "- ROC curves (TPR vs FPR) are good for balanced datasets\n",
    "- Precision-Recall curves are better for imbalanced datasets like yours\n",
    "- For your bankruptcy prediction with few positive cases, precision and recall are more informative than FPR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For your bankruptcy prediction model, a precision-recall curve is particularly valuable given your imbalanced dataset. Here's how to create one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Get probability predictions for the positive class (bankruptcy)\n",
    "y_pred_prob = log_reg.predict_proba(X_test)[:,1]\n",
    "\n",
    "# Calculate precision, recall, and thresholds\n",
    "precision, recall, thresholds = precision_recall_curve(y_test, y_pred_prob)\n",
    "\n",
    "# Calculate average precision (AP) - similar to area under PR curve\n",
    "ap = average_precision_score(y_test, y_pred_prob)\n",
    "\n",
    "# Create the precision-recall curve\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# Plot the precision-recall curve\n",
    "plt.plot(recall, precision, label=f'Logistic Regression (AP = {ap:.3f})')\n",
    "\n",
    "# Plot the baseline (proportion of positive class)\n",
    "baseline = np.sum(y_test) / len(y_test)\n",
    "plt.axhline(y=baseline, color='r', linestyle='--', \n",
    "            label=f'Baseline (No Skill) = {baseline:.3f}')\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision-Recall Curve for Bankruptcy Prediction')\n",
    "\n",
    "# Set axis limits\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "\n",
    "# Add legend and grid\n",
    "plt.legend(loc='best')\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()\n",
    "\n",
    "# Optional: Plot precision and recall as functions of threshold\n",
    "plt.figure(figsize=(8, 6))\n",
    "\n",
    "# We need to add a point at threshold=0 to complete the curve\n",
    "thresholds = np.append(thresholds, 1.0)\n",
    "precision = np.append(precision[:-1], 1.0)\n",
    "recall = np.append(recall[:-1], 0.0)\n",
    "\n",
    "plt.plot(thresholds, precision[:-1], 'b--', label='Precision')\n",
    "plt.plot(thresholds, recall[:-1], 'g-', label='Recall')\n",
    "\n",
    "plt.xlabel('Threshold')\n",
    "plt.ylabel('Score')\n",
    "plt.title('Precision and Recall vs. Threshold')\n",
    "plt.legend()\n",
    "plt.grid(alpha=0.3)\n",
    "\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How to Interpret the Precision-Recall Curve:\n",
    "- The curve: Shows the trade-off between precision and recall at different classification thresholds\n",
    "- Baseline: The horizontal line represents a \"no skill\" model that randomly predicts the positive class at the rate it appears in the dataset\n",
    "- Area under the curve: The higher the average precision (AP), the better the model\n",
    "- Curve shape:\n",
    "- A curve that stays in the upper-right corner (high precision and high recall) indicates a good model\n",
    "- A curve that drops quickly indicates the model struggles to maintain precision as recall increases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Interpretation\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1) SHAP Summary Plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Feature Importance Bar Charts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "3. Coefficient Plots for Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating SHAP Summary Plots from a Pickled Model\n",
    "\n",
    "To create SHAP (SHapley Additive exPlanations) summary plots from your saved pickle model, you'll need to:\n",
    "\n",
    "- Load the pickled model\n",
    "- Create a SHAP explainer for your model\n",
    "- Calculate SHAP values\n",
    "- Generate the summary plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pickle\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Load the pickled model\n",
    "with open('best_model.pkl', 'rb') as file:\n",
    "    best_model = pickle.load(file)\n",
    "\n",
    "# Get a sample of your data for SHAP analysis (using the same X_test you used for evaluation)\n",
    "# If X_test is too large, you might want to sample it\n",
    "X_sample = X_test.copy()\n",
    "if len(X_test) > 1000:  # Optional: limit sample size for faster computation\n",
    "    X_sample = X_test.sample(1000, random_state=42)\n",
    "\n",
    "# Create the SHAP explainer based on your model type\n",
    "# For tree-based models (like RandomForest, XGBoost, etc.)\n",
    "if hasattr(best_model, 'predict_proba') and hasattr(best_model, 'estimators_'):\n",
    "    explainer = shap.TreeExplainer(best_model)\n",
    "# For other models (like logistic regression)\n",
    "else:\n",
    "    # Create a background dataset for the explainer (using training data)\n",
    "    X_background = shap.sample(X_train, 100, random_state=42)  # Sample for efficiency\n",
    "    explainer = shap.KernelExplainer(best_model.predict_proba, X_background)\n",
    "\n",
    "# Calculate SHAP values\n",
    "shap_values = explainer.shap_values(X_sample)\n",
    "\n",
    "# For multi-class models, shap_values will be a list of arrays (one per class)\n",
    "# For binary classification, we typically want the values for class 1 (bankruptcy)\n",
    "if isinstance(shap_values, list):\n",
    "    shap_values_to_plot = shap_values[1]  # Values for class 1 (bankruptcy)\n",
    "else:\n",
    "    shap_values_to_plot = shap_values\n",
    "\n",
    "# Create the SHAP summary plot\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values_to_plot, X_sample, feature_names=X_sample.columns)\n",
    "plt.title('SHAP Summary Plot for Bankruptcy Prediction Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Optional: Create a SHAP bar plot to see average impact magnitude\n",
    "plt.figure(figsize=(10, 8))\n",
    "shap.summary_plot(shap_values_to_plot, X_sample, plot_type='bar', feature_names=X_sample.columns)\n",
    "plt.title('SHAP Feature Importance for Bankruptcy Prediction Model')\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import shap\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "# Assuming you have:\n",
    "# 1. A trained pipeline called 'best_model' from your bankruptcy prediction project\n",
    "# 2. X_train and X_test datasets with your financial indicators\n",
    "# 3. Feature names stored in your dataset\n",
    "\n",
    "# --- Extract Steps from Your Best Pipeline ---\n",
    "preprocessor = best_model.named_steps['preprocessor']  # Adjust if your pipeline has different step names\n",
    "classifier = best_model.named_steps['classifier']  # This might be 'regressor' or another name in your pipeline\n",
    "\n",
    "# --- Transform the Data ---\n",
    "X_train_transformed = preprocessor.transform(X_train)\n",
    "X_test_transformed = preprocessor.transform(X_test)\n",
    "\n",
    "# If your transformed data is not a DataFrame but you have feature names\n",
    "# Create a list of feature names that match your transformed data\n",
    "# This might require extracting column names from your preprocessor if it includes OneHotEncoder or similar\n",
    "feature_names = []  # Fill this with your actual feature names after preprocessing\n",
    "\n",
    "# --- Create a Background Sample for SHAP ---\n",
    "background = shap.sample(X_train_transformed, 100, random_state=42)\n",
    "\n",
    "# --- Create the SHAP Explainer ---\n",
    "# For classification models, you might want to use the predict_proba method instead\n",
    "# to get probability estimates of bankruptcy\n",
    "explainer = shap.KernelExplainer(\n",
    "    lambda x: classifier.predict_proba(x)[:,1],  # For binary classification, focus on positive class\n",
    "    background\n",
    ")\n",
    "\n",
    "# --- Local Explanation (for a single company) ---\n",
    "# Select one observation from the test set\n",
    "observation = X_test_transformed[0:1, :]\n",
    "shap_values = explainer.shap_values(observation)\n",
    "\n",
    "# Initialize the JavaScript visualization\n",
    "shap.initjs()\n",
    "\n",
    "# Create and display a force plot for the selected company\n",
    "force_plot = shap.force_plot(\n",
    "    explainer.expected_value, \n",
    "    shap_values, \n",
    "    features=observation, \n",
    "    feature_names=feature_names, \n",
    "    matplotlib=True\n",
    ")\n",
    "plt.title(\"SHAP Force Plot: Factors Influencing Bankruptcy Prediction\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Global Explanation (for understanding overall model behavior) ---\n",
    "# Compute SHAP values for a sample of the test data\n",
    "X_test_sample = shap.sample(X_test_transformed, 200, random_state=42)\n",
    "shap_values_test = explainer.shap_values(X_test_sample)\n",
    "\n",
    "# Create a summary plot to show overall feature importance\n",
    "plt.figure(figsize=(12, 8))\n",
    "shap.summary_plot(\n",
    "    shap_values_test, \n",
    "    features=X_test_sample, \n",
    "    feature_names=feature_names,\n",
    "    plot_type=\"bar\"  # This creates the feature importance bar chart you asked about\n",
    ")\n",
    "plt.title(\"Feature Importance Based on SHAP Values\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Create a detailed summary plot showing distribution of SHAP values\n",
    "plt.figure(figsize=(12, 10))\n",
    "shap.summary_plot(\n",
    "    shap_values_test, \n",
    "    features=X_test_sample, \n",
    "    feature_names=feature_names\n",
    ")\n",
    "plt.title(\"SHAP Summary Plot: Impact of Features on Bankruptcy Prediction\")\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# --- Additional Visualization: SHAP Dependence Plots ---\n",
    "# For the top 3 most important features (based on your README: debt-to-equity ratio, current ratio, operating cash flow)\n",
    "# Assuming these features are in your dataset and you know their indices after transformation\n",
    "important_features = [\n",
    "    # Replace these indices with the actual indices of your important features\n",
    "    feature_names.index(\"debt_to_equity_ratio\"),\n",
    "    feature_names.index(\"current_ratio\"),\n",
    "    feature_names.index(\"operating_cash_flow\")\n",
    "]\n",
    "\n",
    "for idx in important_features:\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    shap.dependence_plot(\n",
    "        idx, \n",
    "        shap_values_test, \n",
    "        X_test_sample,\n",
    "        feature_names=feature_names\n",
    "    )\n",
    "    plt.title(f\"SHAP Dependence Plot: {feature_names[idx]}\")\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Findings Explanation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dsi_participant",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
